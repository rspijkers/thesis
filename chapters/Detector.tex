This chapter describes the ALICE experiment, focusing on the detector first but also describing the O2 framework (both offline and online components, but probably mostly offline).

\section{The Large Hadron Collider (LHC)}
\label{sec:LHC}
  The LHC \cite{LHCMachine} is the world's largest particle accelerator and collider, able to reach center of mass energies of up to 13.6 TeV in proton-proton collisions. A circular accelerator with a circumference of 27 km, it spans the border between Switzerland and France near Geneva. It is part of the CERN\footnote{The acronym stands for ``Conseil Européen pour la Recherche Nucléaire''} (European Organization for Nuclear Research) accelerator complex, a schematic of which is shown in Figure \ref{fig:LHC}. The first serious considerations for the LHC started in the early 1980s, even before its predecessor, the Large Electron Positron collider (LEP), started running. \cite{LHCFacts} Construction of the LHC started in 1998, reusing the LEP tunnel and some of its infrastructure, and finished in 2008. The first collisions were recorded in 2009, and the LHC has been in operation ever since, with periodic upgrades and maintenance breaks. 

  \begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Detector/AcceleratorComplex.png}
    \caption{Schematic layout of the LHC accelerator complex, showing the different accelerators used to bring protons and heavy ions up to LHC injection energy, as well as accelerators related to other experiments (image from \cite{AcceleratorComplex}).}
    \label{fig:LHC}
  \end{figure}

  The hadrons that are collided in the LHC have their energy increased in a series of smaller accelerators before being injected into the LHC. In the case of protons, H$^-$ ions are first accelerated to 160 MeV in the Linac4 before being stripped of their electrons and injected into the Proton Synchrotron Booster (PSB). Here they are accelerated to 2 GeV before being sent to the Proton Synchrotron (PS), Super Proton Synchrotron (SPS), and finally the LHC itself which they enter at an energy of 450 GeV. \cite{AcceleratorComplexWebsite} With the exception of the Linac4 which uses a linear acceleration method, all accelerators in the chain are circular synchrotrons. Particles in synchrotrons are kept in a circular path using powerful superconducting magnets, while other magnets increase the energy of the particles step by step using radiofrequency cavities, like pushing a child on a swing.

  There are four main experiments at the LHC: ATLAS \cite{ATLAS}, CMS \cite{CMS}, ALICE \cite{ALICE}, and LHCb \cite{LHCb}. ATLAS and CMS are the largest of the experiments, general-purpose detectors designed to study a wide range of physics phenomena. They are also the most well-known experiments, having discovered the Higgs boson in 2012. \cite{ATLASHiggs,CMSHiggs} LHCb is a smaller experiment focused on studying the properties of b-quarks and CP violation. \cite{LHCb} ALICE (A Large Ion Collider Experiment) is specifically designed to study heavy-ion collisions, with the goal of understanding the properties of the quark-gluon plasma (QGP), a state of matter that existed shortly after the Big Bang. \cite{ALICE}
  
  As of the time of this thesis, the LHC is in its third data-taking period (Run 3), which started in 2022 after a two-year long upgrade and maintenance break (Long Shutdown 2, LS2). This upgrade finally enabled the LHC to reach its maximum collision energy of 13.6 TeV in proton-proton collisions, as well as drastically increasing the rate of collisions (luminosity). This increase in luminosity enables research that is heavily limited by the available statistics, such as studies that involve rare processes and measurements that require high precision. 

\section{The ALICE detector}

  While the LHC went through its LS2 upgrade, the ALICE detector also underwent a significant upgrade to prepare for Run 3 and beyond. \cite{ALICEUpgrades} As large parts of the detector were replaced or significantly modified, the new detector is sometimes called ALICE 2 (or 2.0) to distinguish it from the detector used in Run 1 and Run 2. Because this thesis concerns data taken in Run 3 we refer to the upgraded detector simply as ALICE, a schematic of which is shown in Figure \ref{fig:ALICE}. The bulk of the detector is built in a cylindrical shape around the beam pipe, centered at the interaction point (IP). The detector is approximately 16 m tall and 26 m long, with a total weight of around 10,000 tons. It consists of several sub-detectors, each designed to perform specific tasks such as tracking, particle identification (PID), and calorimetry. The main tracking detectors used in this thesis are the Inner Tracking System (ITS) and the Time Projection Chamber (TPC), both located in the central barrel of the detector. The central barrel is surrounded by a solenoidal magnet that provides a uniform magnetic field of 0.5 T along the beam axis, allowing for momentum measurements via track curvature. Other sub-detectors include the Time Of Flight (TOF) detector for PID, the Electromagnetic Calorimeter (EMCal) for measuring the energy of electrons and photons, and the Forward Multiplicity Detector (FMD) and V0 detectors for triggering and event characterization.

  \begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{example-image-a}
    \caption{Schematic layout of the ALICE detector after the LS2 upgrade (image from \cite{ALICEUpgrades}). 
    \rs{TODO: access this picture from the ALICE repository, website down atm}}
    \label{fig:ALICE}
  \end{figure}

  \subsection{Inner Tracking System}

    The Inner Tracking System (ITS) \cite{ITS} is the innermost sub-detector of ALICE located closest to the interaction point, and is a completely new detector compared to Runs 1 and 2. It's primary function is to reconstruct charged-particle tracks and the primary vertex. It consists of 7 layers of Monolithic Active Pixel Sensors (MAPS) arranged in cylindrical layers around the beam pipe. The first three layers are very close to the beam pipe, at radii of 2.3 to 3.9 cm, which is crucial for determining the primary vertex position with high resolution. The middle two layers are located at 19.6 and 24.5 cm, while the outermost two layers are at 34.4 and 39.3 cm. A layer of the ITS consists of multiple staves parallel to the beam axis, each containing the mechanical support, cooling system, and the Hybrid Integrated Circuits (HICs) that hold the MAPS chips. A HIC contains multiple MAPS chips, 9 for the inner layers 14 for the outer layers. The inner 3 layers only contain one HIC per stave, but the chips have individual outputs that are read out separately. This is necessary to deal with the high occupancy close to the beam pipe, with the added benefit that if one chip fails the rest of the layer can still function. The outer 4 layers contain multiple HICs per stave, with the chips on a HIC connected in subgroups of 7 chips that share a common output. This reduces both the number of copper links, therefor reducing the material budget, and the power consumption. 

    \rs{add picture of ITS here}

    The pixels themselves have a size of 27 x 29 $\mu$m$^2$, allowing for very precise tracking with a spatial resolution of around 5 $\mu$m in the transverse plane. The ITS covers a pseudorapidity range of $|\eta| < 1.3$ and has full azimuthal coverage. 

    Because the ITS is located so close to the interaction point and this thesis deals with weakly decaying particles, most of the tracks originating from these decays will only start after the 3 innermost layers of the ITS. Therefor, although the ITS plays an important role in the reconstruction of these tracks, another tracking detector is needed to provide additional tracking information and improve the resolution. 

  \subsection{Time Projection Chamber}

    The Time Projection Chamber (TPC) is a tracking and PID detector that surrounds the ITS and covers a pseudorapidity range of $|\eta| < 0.9$. It is a large cylindrical detector, 5 m in length and 5 m in diameter,filled with a gas mixture of Ne-CO$_2$-N$_2$ (90-10-5). There is a central electrode at the center ($z = 0$) of the TPC that creates a uniform electric field along the beam axis, and the endcaps of the cylinder are split into 18 equal parts that each have an inner and outer readout chamber. When a charged particle passes through the TPC, it ionizes the gas along its path, creating free electrons that drift towards the endcaps under the influence of the electric field. The drift time of the electrons is used to determine the position of the particle along the beam axis ($z$-coordinate), while the position of the particle in the $r\varphi$ plane is simply given by the shape of the signal on the readout pads. 

    \rs{add picture of TPC here}

    On top of providing tracking information, the TPC also allows for particle identification via the measurement of the energy the particle loses as it traverses and interacts with the gas. This energy loss, d$E$/d$x$, is described by the Bethe-Bloch formula \cite{ThomsonMPP} which depends on the particle mass, charge, and velocity. By determining the momentum of the particle from the curvature of its track in the magnetic field and measuring its d$E$/d$x$ in the TPC, one can infer the mass of the particle and therefor identify it.\footnote{The astute reader may note that we have note determined the charge of the particle. However, most particles that traverse the TPC are singly charged, and this definitely holds for the particles studied in this thesis. The polarity of the charge can simply be determined from the direction of curvature of the track in the magnetic field.}
    
    \rs{probably add a figure showing bethe bloch curves here}

    Although the main design of the TPC is very similar to the one used in Runs 1 and 2, it has been upgraded to allow for continuous readout as opposed to operating based on a triggered system as was done before. This was necessary to deal with the increased collision rates in Run 3, as a triggered operating mode is inherently limited by the time it takes an electron to drift to the endcaps (up to 100 $\mu$s for electrons originating near the central electrode). This corresponds to a readout frequency of 10 kHz (assuming no downtime in between triggered events), while in Run 3 the aspired interaction rate for Pb-Pb collisions is 50 kHz, and even higher for pp collisions. Suffice to say, a triggered readout would not be able to keep up with the data rates in Run 3. However, continuous readout also exclude the use of active ion gating grid as was used in Runs 1 and 2. This gate prevents the positively charged ions created in the amplifying stage of the readout from flowing back into the drift volume. To remedy this, the Multi-Wire Proportional Chambers (MWPCs) were upgraded to Gas Electron Multiplier (GEM) \cite{GEM} based readout chambers. The GEMs can be optimized to have significantly lower ion backflow compared to MWPCs, on top of this they can be stacked in multiple layers to further reduce the ion backflow while still providing sufficient gain for the readout. 

  \subsection{FIT}
  \rs{maybe reduce this to a small paragraph in case I don't need to describe multiplicity in detail.}

\section{The O2 framework}

  The software framework used for data processing in ALICE Run 3 and beyond is called \otwo (Online-Offline) \cite{O2}, named after the fact that we use the same framework for both real-time data processing (online) and offline data analysis. \rs{perhaps link/refer to the github pages here, or at least mention that it is publicly available?} An overhaul of the previous ALICE software framework was necessary to deal with the increased data rates and volumes expected in Run 3. The \otwo framework is written in c++ and interfaces with ROOT, another software framework widely used in high-energy physics for data analysis and visualization. \cite{ROOT} 

  The philosophy behind the \otwo framework to have a modular architecture that uses and produces flat data structures. The different modules are called workflows, or sometimes (analysis) tasks, and the flat data structures are Apache Arrow tables \cite{ApacheArrow}. Workflows can be chained together to form more complex data processing pipelines, allowing for flexibility and reusability of code. The use of flat data structures allows for efficient data processing and storage. A row in a table corresponds to a single element (such as a track) and the columns to the properties (such as the transverse momentum). An important feature of this flat data structure is that one may correspond between tables via indices. For example, instead of having a nested structure like a list of collisions where each collision contains a list of tracks, we have one table for collisions and one table for tracks. The track table then contains a column with the index of the collision it belongs to - this index directly references the row of the parent collision in the collision table. 

  \subsection{online}
  
    The raw detecter outputs such as the signals from the ITS chips and TPC pads are first processed in real-time by the online component of the \otwo framework. This is done on First Level Processors (FLPs) that receive the data via fiber optic cables directly from the different subdetectors. The data flows into the FLPs at an extremely high rate, up to 3.4 TB/s in total for all detectors combined, the FLPs are responsible for performing an initial compression of the data as well as possibly some first calibration tasks. The compressed data is then sent to the Event Processing Nodes (EPNs) via a high-speed network for further processing, such as further reducing the data volume and a first online reconstruction pass. Even though the events will ultimately be reprocessed offline, an online reconstruction is necessary to compress the data and to perform Quality Control/Assurance (QC/QA). Another important task of the EPNs is to extract calibration data from the raw detector outputs, which are used in the asynchronous (offline) reconstruction passes. 

  \subsection{offline}

    The data used in almost all analyses, including the one presented in this thesis, is reconstructed offline using the asynchronous component of the \otwo framework. A specific reconstruction of the raw data using a defined set of calibration objects and algorithms is called an asynchronous pass, apass for short. This data contains reconstructed events and tracks as well as their respective properties. \rs{expand, but I'm not sure exactly in how much detail I have to go?}